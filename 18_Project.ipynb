{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-18 Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samarth1410/Samarth_FMML/blob/main/18_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many to Many RNNs\n",
        "\n",
        "These types of networks take a sequence as an input and give a sequence as an output. It can be used in problems like machine translation, named entity recognition, POS tagging and others.\n",
        "\n",
        "In this project you would work on different types of RNNs on the task of POS tagging. "
      ],
      "metadata": {
        "id": "wR1-tzFtmx5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "Mc4T5BKsIjz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waY0Fb_AmVB1"
      },
      "outputs": [],
      "source": [
        "## We will use Treebank from NLTK as dataset\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load POS tagged corpora from NLTK\n",
        "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
        "brown_corpus = brown.tagged_sents(tagset='universal')\n",
        "tagged_sentences = treebank_corpus + brown_corpus\n"
      ],
      "metadata": {
        "id": "gSTdU2tqIYmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sentences: \" + str(len(tagged_sentences)))\n",
        "tagged_sentences[0]"
      ],
      "metadata": {
        "id": "Zdp_-ESKLTnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a many-to-many problem, each data point will be a different sentence of the corpora.\n",
        "\n",
        "Each data point will have multiple words in the input sequence. This is what we will refer to as X.\n",
        "\n",
        "Each word will have its correpsonding tag in the output sequence. This what we will refer to as Y.\n",
        "\n"
      ],
      "metadata": {
        "id": "2i3S452lmwqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "\n",
        "for sentence in tagged_sentences:\n",
        "    X_sentence = []\n",
        "    Y_sentence = []\n",
        "    for entity in sentence:         \n",
        "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "        \n",
        "    X.append(X_sentence)\n",
        "    Y.append(Y_sentence)\n"
      ],
      "metadata": {
        "id": "dSUwl-pWL-4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
        "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n"
      ],
      "metadata": {
        "id": "L-CLIg5oMBeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
        "print(\"Vocabulary size: {}\".format(num_words))\n",
        "print(\"Total number of tags: {}\".format(num_tags))"
      ],
      "metadata": {
        "id": "izQ9_aO1MD0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Task - 1\n",
        "## Vectorize each sentence and pad each sequence to a fixed length"
      ],
      "metadata": {
        "id": "jaj5o3tlMKMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "C_rGKjQcM6a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "                                                # fit tokeniser on data\n",
        "                                                # use the tokeniser to encode input sequence"
      ],
      "metadata": {
        "id": "xHyfAoHIM2kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Task - 2 \n",
        "## Convert Y to categorical and pad it as input\n",
        "tag_tokenizer = Tokenizer()\n"
      ],
      "metadata": {
        "id": "W8E9jODFMqa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Padding\n",
        "#X_encoded is the encoded form X from Task-1\n",
        "MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
        "\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "# Pad for Y\n",
        "X, Y = X_padded, Y_padded\n"
      ],
      "metadata": {
        "id": "IXc87UyONM1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Y to categorical"
      ],
      "metadata": {
        "id": "H58Fbv1TNZ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n"
      ],
      "metadata": {
        "id": "pzym9XOSNen5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Split data in training and testing \n",
        "TEST_SIZE = 0.15\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n"
      ],
      "metadata": {
        "id": "bmk6GtXZMhoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TRAINING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_train.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"TESTING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_test.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_test.shape))\n"
      ],
      "metadata": {
        "id": "zoOtnDLuMpcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = Y.shape[2]"
      ],
      "metadata": {
        "id": "QZJ8foUQNoZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "t9uxggfrNvzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "ZBEE_iP7PsIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task - 3 Complete the two lines"
      ],
      "metadata": {
        "id": "VM0MaaO9BdxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = Sequential()\n",
        "\n",
        "# create embedding layer - usually the first layer in text problems\n",
        "rnn_model.add(Embedding(num_words + 1,         # vocabulary size - number of unique words in data\n",
        "                        output_dim    =  300,          # length of vector with which each word is represented\n",
        "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                        trainable     =  False                    # False - don't update the embeddings\n",
        "))\n",
        "\n",
        "# add an any RNN layer which contains 64 RNN cells\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n"
      ],
      "metadata": {
        "id": "80yPeIRGNrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])\n"
      ],
      "metadata": {
        "id": "pS0uwKlCNu3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.summary()\n"
      ],
      "metadata": {
        "id": "a90HE-aROobD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=256, epochs=10)\n"
      ],
      "metadata": {
        "id": "xRuJr-oaOp3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "dV9RiL92ArQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rnn_training.history['acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XO7E0Z9QOt0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = rnn_model.evaluate(X_test, Y_test, verbose = 1)\n",
        "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
      ],
      "metadata": {
        "id": "StG9D6m-Gj1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Task - 4 How did turning the trainable parameter in Embedding layer into True effect the performance?"
      ],
      "metadata": {
        "id": "mwUWYtbWBxzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Task - 5 How else can you improve the accuracy? "
      ],
      "metadata": {
        "id": "EkBr6IuKHKUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Task - 6 Use other RNNs present in Keras like LSTM, GRU, BiLSTMs, BiGRU and compare any three models with RNNs "
      ],
      "metadata": {
        "id": "Eg4KYjksGIw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}